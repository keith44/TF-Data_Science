{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So far when explaining how regression works, we've said that it finds the model of best fit by minimizing the squared distance between each datapoint and the line of fit.  Squaring the distance removes concerns about positive vs negative signs, and has a heavier penalty for larger distances.  \n",
    "\n",
    "The cost function for a linear regression model $y_i = \\alpha + \\beta x_i$ is:\n",
    "\n",
    "$$\\frac1{n}\\sum_{i=1}^n(y_i-(\\alpha + \\beta x_i))^2$$\n",
    "\n",
    "where $\\alpha + \\beta x_i$ is the prediction of the model $\\alpha + \\beta x$ for predictors $x_i$, $y_i$ is the actual outcome value, and $n$ is the number of distances being summed.\n",
    "\n",
    "For many linear regressions, the model is sufficiently simple that the true minimum of the cost function can be calculated by solving a system of equations.  However, many other models that we will encounter from this point forward are _too complex_ to be solved for a true minimum.  For those models it's useful to use an iterative algorithm that starts from a random set of parameters and slowly works toward optimizing the cost function.\n",
    "\n",
    "One such algorithm is **gradient descent**, which iteratively minimizes the cost function using derivatives.  This approach is robust and flexible, and can be applied to basically any differentiable function.\n",
    "\n",
    "Now we're going to get into the nuts-and-bolts of how gradient descent works (and what differentiable functions are). Hold on to your hats, we're gonna do some calculus!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "After learning about PCA, you should be comfortable with the idea of data as a multi-dimensional space.  When optimizing a linear regression, the dimensions of the space correspond to the number of parameters in the equation, plus the error function we are trying to minimize.  So a model $y_i = \\alpha + \\beta x_i$ with two parameters would yield a three-dimensional space.  Within that space is a *surface* made up of all  possible combinations of parameter values, and the error values that result when we plug those parameters into the cost function.  (In a two-dimensional space, we have lines.  In three dimensions and higher, we have surfaces.)\n",
    "\n",
    "The gradient descent algorithm works iteratively by picking a location on the surface defined by a combination of parameter values, calculating the direction from that point with the steepest 'downhill' gradient, and then moving 'downhill' a set distance.  Then the algorithm picks up the new parameter values of that location on the surface, re-calculates the direction of 'downhill' and moves a set distance again.  The algorithm will repeat this until it finds a location on the surface where all possible gradients away from that location are \"uphill\": in other words, where all other possible combinations of parameters result in higher error values.  The parameter values that define the location at the lowest point of the space represent the \"optimized\" solution to the cost function, and are what the regression returns as a solution.\n",
    "\n",
    "The direction of \"downhill\" is determined by differentiating the cost function and taking the partial derivative of each parameter of the regression equation.  A function is \"differentiable\" if a derivative can be calculated at each value of the function.  A derivative, in turn, is a measure of how sensitive a quantity is to change in another quantity.  In other words, if there is a function $f$ that contains parameters $x$ and $y$, the partial derivative for $x$ (expressed as $\\frac{\\partial}{\\partial y}$) will tell us how much $y$ will change for each unit change in $x$.  We could also calculate $\\frac{\\partial}{\\partial x}$, to find out how much a one-unit change in $y$ will impact $x$.\n",
    "\n",
    "For our two-parameter regression line model, the derivatives are:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\alpha} =\\frac2n \\sum_{i=1}^n - (y^i-(\\alpha + \\beta x_i) )$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta} =\\frac2n \\sum_{i=1}^n - x_i(y^i-(\\alpha + \\beta x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Decision-points in Gradient Descent\n",
    "\n",
    "There are three elements of the gradient descent algorithm that require decisions on the part of the operator.  \n",
    "\n",
    "\n",
    "### What are the starting values of the parameters?   \n",
    "\n",
    "Many implementations will start by setting all parameters to zero.  However, this isn't a requirement of the algorithm, and sometimes other starting points may be desirable.\n",
    "\n",
    "\n",
    "### How far do we \"move downhill\" after each iteration?\n",
    "\n",
    "Also called the \"learning rate.\"  A too-small learning rate means the model will be computationally inefficient and take a long time to converge (stop).  A too-large learning rate can result in overshooting the target minimum, resulting in a model that _never_ converges.  Again, most algorithm implementations have pre-determined criteria for setting the learning rate, but these can also be set manually.\n",
    "\n",
    "\n",
    "### When do we stop?\n",
    "\n",
    "In the description above, it sounds like the model runs until it reaches the \"optimal\" solution.  In reality, this isn't computationally efficient.  As the gradient flattens out and we get closer and closer to the minimum value of the error, each iteration of the algorithm will result in a smaller and smaller change in the error.  This can get really slow.  Typically some \"minimal acceptable change\" is decided on a-priori – once the change in error from iteration n-1 to iteration n is smaller than the threshold, the algorithm stops.  To prevent an algorithm that never stops, there is usually also a maximum number of permitted iterations before the gradient stops, even if it hasn't achieved a change under the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guts of Gradient Descent\n",
    "\n",
    "Let's walk through programming a gradient descent algorithm in Python.  There are packages that will do this for you, but for now we'll try it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients from sklearn: \n",
      " [[1.99349093]]\n",
      "\n",
      "Intercept from sklearn: \n",
      " [0.48271267]\n",
      "\n",
      "Coefficients from gradient descent algorithm: \n",
      " 1.993473024979401\n",
      "\n",
      "Intercept from gradient descent algorithm: \n",
      " 0.4826545964041896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 363\n"
     ]
    }
   ],
   "source": [
    "## Cost function for the linear regression that we will try to optimize.\n",
    "def LR_cost_function (alpha, beta, x, y):\n",
    "    '''Return the cost for a given line and data.\n",
    "    \n",
    "    Alpha and beta are the coeficients that describe the fit line line, while\n",
    "    x and y are lists or arrays with the x and y value of each data point.\n",
    "    '''\n",
    "    error = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        point_error = (y[i] - (alpha + beta * x[i])) ** 2\n",
    "        error += point_error\n",
    "    return error / n\n",
    "\n",
    "\n",
    "# Function we'll call each iteration (or step) of the gradient algorithm.\n",
    "def step (alpha_cur, beta_cur, learning_rate, x, y):\n",
    "    '''Move downhill from a current cost function to a new, more optimal one.'''\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        # Partial derivative of the intercept.\n",
    "        point_alpha = -(2 / n) * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        alpha += point_alpha\n",
    "        \n",
    "        # Partial derivative of the slope.\n",
    "        point_beta = -(2 / n) * x[i] * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        beta += point_beta\n",
    "        \n",
    "    new_alpha = alpha_cur - learning_rate * alpha \n",
    "    new_beta = beta_cur - learning_rate * beta\n",
    "    return [new_alpha, new_beta]\n",
    "\n",
    "# These constants correspond to the decision-points described above.\n",
    "# How many steps to take.\n",
    "stop = 1000\n",
    "\n",
    "# How far to move with each step.\n",
    "learning_rate = .005\n",
    "\n",
    "# Starting values for intercept and slope \n",
    "alpha_start = 0\n",
    "beta_start = 0\n",
    "\n",
    "# Time to make some data!\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = x * 2 + np.random.sample(100)\n",
    "\n",
    "# Fit an true minimum regression using solved equations.\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "\n",
    "print('\\nCoefficients from sklearn: \\n', regr.coef_)\n",
    "print('\\nIntercept from sklearn: \\n', regr.intercept_)\n",
    "\n",
    "\n",
    "# Now fit an iteratively optimized regression using your custom gradient\n",
    "# descent algorithm.\n",
    "\n",
    "# Storing each iteration to inspect later.\n",
    "all_error=[]\n",
    "for i in all_error:\n",
    "    quit = error - i\n",
    "    print(quit)\n",
    "# Provide starting values.\n",
    "alpha = alpha_start\n",
    "beta = beta_start\n",
    "\n",
    "#Run the algorithm.\n",
    "for iter in range(stop):\n",
    "    \n",
    "    # Take a step, assigning the results of our step function to feed into\n",
    "    # the next step.\n",
    "    alpha, beta = step(alpha, beta, learning_rate, x, y)\n",
    "    \n",
    "    # Calculate the error.\n",
    "    error = LR_cost_function(alpha, beta, x, y)\n",
    "    \n",
    "    # Store the error to instpect later.\n",
    "    all_error.append(error)\n",
    "a = []\n",
    "for i in all_error:\n",
    "    if i-error > .001:\n",
    "        a.append(i-error)\n",
    "stop = len(a)\n",
    "    \n",
    "print('\\nCoefficients from gradient descent algorithm: \\n', beta)\n",
    "print('\\nIntercept from gradient descent algorithm: \\n', alpha)\n",
    "\n",
    "plt.plot(all_error, 'o', ms=.4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error scores for each iteration')\n",
    "plt.show()\n",
    "\n",
    "print ('iterations',len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Things Get Messy\n",
    "\n",
    "Linear regression is a good way to introduce the gradient descent algorithm because there is only one minimum – one absolute best solution.  In other algorithms, however, there may be both a global minimum (the lowest possible value over the entire surface) and many local minima, areas on the surface that are lower than the surface around them.\n",
    "\n",
    "![local and global minima and maxima](assets/maxima_and_minima.svg)\n",
    "\n",
    "When using the gradient descent algorithm with models that have local minima the algorithm can get 'caught' in one and converge on a less-than-optimal solution.  One way to avoid this is to run the algorithm multiple times with different starting values.\n",
    "\n",
    "Still a bit confused? [This](http://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html) is a useful resource for another explanation.\n",
    "\n",
    "## Stopping rules\n",
    "\n",
    "In the implementation programmed above, the only stopping rule involves the number of iterations.  As you can see from the plot above, this might be a bit inefficient in this case.  Modify the code above by adding a stopping threshold so that the algorithm stops when the difference in error between two successive iterations is less than .001.  With that rule, how many iterations do you need before you stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your gradient descent algorithm with stopping threshold here.\n",
    "a = []\n",
    "for i in all_error:\n",
    "    if i-error > .001:\n",
    "        a.append(i-error)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.315276320831301,\n",
       " 5.175477552966705,\n",
       " 5.0394145856232875,\n",
       " 4.906986386311343,\n",
       " 4.77809467903628,\n",
       " 4.652643868613664,\n",
       " 4.53054096707175,\n",
       " 4.411695522083783,\n",
       " 4.296019547373834,\n",
       " 4.18342745504163,\n",
       " 4.073835989753291,\n",
       " 3.967164164746306,\n",
       " 3.863333199598617,\n",
       " 3.7622664597130067,\n",
       " 3.6638893974693123,\n",
       " 3.568129494998435,\n",
       " 3.4749162085331515,\n",
       " 3.3841809142922816,\n",
       " 3.2958568558557184,\n",
       " 3.2098790929891137,\n",
       " 3.126184451878196,\n",
       " 3.0447114767336676,\n",
       " 2.965400382728893,\n",
       " 2.8881930102334556,\n",
       " 2.8130327803068322,\n",
       " 2.7398646514173035,\n",
       " 2.668635077352311,\n",
       " 2.5992919662872644,\n",
       " 2.531784640980865,\n",
       " 2.4660638000657595,\n",
       " 2.4020814804043136,\n",
       " 2.33979102048005,\n",
       " 2.279147024796174,\n",
       " 2.2201053292533452,\n",
       " 2.162622967479666,\n",
       " 2.106658138086603,\n",
       " 2.0521701728252464,\n",
       " 1.9991195056180806,\n",
       " 1.947467642442064,\n",
       " 1.8971771320395527,\n",
       " 1.8482115374341832,\n",
       " 1.8005354082295164,\n",
       " 1.7541142536688505,\n",
       " 1.708914516435153,\n",
       " 1.6649035471707487,\n",
       " 1.6220495796968657,\n",
       " 1.5803217069137343,\n",
       " 1.5396898573624844,\n",
       " 1.5001247724305682,\n",
       " 1.4615979841829638,\n",
       " 1.4240817938019097,\n",
       " 1.3875492506183762,\n",
       " 1.3519741317189655,\n",
       " 1.3173309221123817,\n",
       " 1.2835947954400186,\n",
       " 1.2507415952157013,\n",
       " 1.218747816579975,\n",
       " 1.1875905885547522,\n",
       " 1.1572476567845593,\n",
       " 1.127697366750943,\n",
       " 1.0989186474470158,\n",
       " 1.0708909954994645,\n",
       " 1.0435944597256883,\n",
       " 1.0170096261140869,\n",
       " 0.99111760321584,\n",
       " 0.9659000079368529,\n",
       " 0.9413389517188391,\n",
       " 0.9174170270988402,\n",
       " 0.8941172946367466,\n",
       " 0.8714232702007085,\n",
       " 0.84931891260057,\n",
       " 0.8277886115597608,\n",
       " 0.8068171760163183,\n",
       " 0.7863898227440047,\n",
       " 0.7664921652846881,\n",
       " 0.7471102031834429,\n",
       " 0.7282303115180375,\n",
       " 0.7098392307147077,\n",
       " 0.6919240566423546,\n",
       " 0.6744722309774991,\n",
       " 0.6574715318325572,\n",
       " 0.6409100646401915,\n",
       " 0.6247762532867087,\n",
       " 0.6090588314876488,\n",
       " 0.5937468343989141,\n",
       " 0.5788295904569716,\n",
       " 0.5642967134418244,\n",
       " 0.5501380947566461,\n",
       " 0.5363438959181122,\n",
       " 0.5229045412516531,\n",
       " 0.5098107107860017,\n",
       " 0.4970533333415538,\n",
       " 0.484623579807232,\n",
       " 0.47251285660066894,\n",
       " 0.4607127993066888,\n",
       " 0.44921526648918636,\n",
       " 0.4380123336716473,\n",
       " 0.42709628748168466,\n",
       " 0.4164596199550919,\n",
       " 0.40609502299503997,\n",
       " 0.3959953829821556,\n",
       " 0.38615377553135266,\n",
       " 0.37656346039139094,\n",
       " 0.36721787648324483,\n",
       " 0.3581106370734845,\n",
       " 0.3492355250789636,\n",
       " 0.3405864884992211,\n",
       " 0.33215763597309145,\n",
       " 0.32394323245613094,\n",
       " 0.3159376950155387,\n",
       " 0.3081355887393644,\n",
       " 0.30053162275687056,\n",
       " 0.293120646367004,\n",
       " 0.28589764527202277,\n",
       " 0.2788577379133926,\n",
       " 0.27199617190716435,\n",
       " 0.26530832057610054,\n",
       " 0.25878967957591315,\n",
       " 0.25243586361303155,\n",
       " 0.24624260325140462,\n",
       " 0.24020574180589382,\n",
       " 0.23432123231990037,\n",
       " 0.22858513462491348,\n",
       " 0.22299361247975075,\n",
       " 0.21754293078730624,\n",
       " 0.21222945288669032,\n",
       " 0.2070496379187049,\n",
       " 0.20200003826264784,\n",
       " 0.19707729704249882,\n",
       " 0.19227814570059715,\n",
       " 0.18759940163696093,\n",
       " 0.18303796591246546,\n",
       " 0.17859082101413132,\n",
       " 0.17425502868083015,\n",
       " 0.1700277277877617,\n",
       " 0.16590613228809648,\n",
       " 0.16188752921022564,\n",
       " 0.1579692767091065,\n",
       " 0.15414880217022142,\n",
       " 0.15042360036472346,\n",
       " 0.14679123165437036,\n",
       " 0.14324932024489068,\n",
       " 0.13979555248646422,\n",
       " 0.13642767522003413,\n",
       " 0.13314349416819946,\n",
       " 0.12994087236947893,\n",
       " 0.12681772865476412,\n",
       " 0.1237720361648113,\n",
       " 0.1208018209076597,\n",
       " 0.1179051603548889,\n",
       " 0.11508018207565682,\n",
       " 0.1123250624074931,\n",
       " 0.10963802516285098,\n",
       " 0.10701734037043997,\n",
       " 0.10446132305040076,\n",
       " 0.10196833202239841,\n",
       " 0.09953676874574163,\n",
       " 0.0971650761906587,\n",
       " 0.09485173773988367,\n",
       " 0.09259527611972973,\n",
       " 0.09039425235985088,\n",
       " 0.08824726478091191,\n",
       " 0.08615294800941159,\n",
       " 0.08410997201892051,\n",
       " 0.08211704119701921,\n",
       " 0.08017289343724024,\n",
       " 0.07827629925533386,\n",
       " 0.07642606092920107,\n",
       " 0.07462101166185225,\n",
       " 0.07286001476676732,\n",
       " 0.07114196287505106,\n",
       " 0.06946577716379371,\n",
       " 0.06783040660506282,\n",
       " 0.0662348272349694,\n",
       " 0.06467804144226291,\n",
       " 0.06315907727592951,\n",
       " 0.06167698777127803,\n",
       " 0.06023085029401523,\n",
       " 0.058819765901823445,\n",
       " 0.057442858722967755,\n",
       " 0.05609927535147344,\n",
       " 0.05478818425842574,\n",
       " 0.05350877521895625,\n",
       " 0.05226025875449461,\n",
       " 0.05104186558986992,\n",
       " 0.049852846124866346,\n",
       " 0.048692469919838,\n",
       " 0.04756002519500804,\n",
       " 0.04645481834308085,\n",
       " 0.04537617345480954,\n",
       " 0.04432343185716983,\n",
       " 0.0432959516637999,\n",
       " 0.04229310733737779,\n",
       " 0.04131428926361283,\n",
       " 0.04035890333653999,\n",
       " 0.03942637055481407,\n",
       " 0.03851612662870407,\n",
       " 0.03762762159750524,\n",
       " 0.03676031945708505,\n",
       " 0.035913697797291866,\n",
       " 0.03508724744896213,\n",
       " 0.034280472140267124,\n",
       " 0.033492888162149806,\n",
       " 0.03272402404260662,\n",
       " 0.031973420229577665,\n",
       " 0.031240628782214175,\n",
       " 0.030525213070298665,\n",
       " 0.02982674748159972,\n",
       " 0.0291448171369474,\n",
       " 0.02847901761282394,\n",
       " 0.027828954671268308,\n",
       " 0.027194243996897377,\n",
       " 0.02657451094085568,\n",
       " 0.025969390271505757,\n",
       " 0.025378525931680523,\n",
       " 0.02480157080232269,\n",
       " 0.024238186472338025,\n",
       " 0.023688043014498894,\n",
       " 0.023150818767235917,\n",
       " 0.022626200122159157,\n",
       " 0.022113881317158315,\n",
       " 0.02161356423493034,\n",
       " 0.021124958206791764,\n",
       " 0.02064777982163439,\n",
       " 0.02018175273988626,\n",
       " 0.019726607512345853,\n",
       " 0.019282081403758353,\n",
       " 0.018847918221009757,\n",
       " 0.01842386814581301,\n",
       " 0.018009687571769212,\n",
       " 0.0176051389456868,\n",
       " 0.017209990613044096,\n",
       " 0.016824016667487257,\n",
       " 0.01644699680425518,\n",
       " 0.01607871617742694,\n",
       " 0.015718965260890463,\n",
       " 0.015367539712934752,\n",
       " 0.015024240244367076,\n",
       " 0.0146888724900648,\n",
       " 0.01436124688386771,\n",
       " 0.014041178536724597,\n",
       " 0.013728487118006519,\n",
       " 0.013422996739904228,\n",
       " 0.013124535844826105,\n",
       " 0.012832937095719393,\n",
       " 0.012548037269235673,\n",
       " 0.012269677151667227,\n",
       " 0.0119977014375785,\n",
       " 0.011731958631064396,\n",
       " 0.011472300949563377,\n",
       " 0.011218584230160075,\n",
       " 0.010970667838310091,\n",
       " 0.010728414578924372,\n",
       " 0.01049169060975011,\n",
       " 0.010260365356987503,\n",
       " 0.010034311433085308,\n",
       " 0.009813404556654498,\n",
       " 0.009597523474447839,\n",
       " 0.009386549885348566,\n",
       " 0.009180368366316333,\n",
       " 0.008978866300238864,\n",
       " 0.008781933805639353,\n",
       " 0.008589463668190522,\n",
       " 0.008401351273987986,\n",
       " 0.008217494544537338,\n",
       " 0.008037793873409069,\n",
       " 0.007862152064518818,\n",
       " 0.007690474271988504,\n",
       " 0.007522667941549446,\n",
       " 0.007358642753444555,\n",
       " 0.007198310566792487,\n",
       " 0.00704158536537354,\n",
       " 0.0068883832048025895,\n",
       " 0.006738622161050284,\n",
       " 0.006592222280278365,\n",
       " 0.006449105529955923,\n",
       " 0.006309195751220992,\n",
       " 0.006172418612456695,\n",
       " 0.0060387015640495645,\n",
       " 0.005907973794299165,\n",
       " 0.005780166186449484,\n",
       " 0.00565521127681308,\n",
       " 0.005533043213957534,\n",
       " 0.005413597718930449,\n",
       " 0.0052968120464916835,\n",
       " 0.005182624947329989,\n",
       " 0.005070976631236623,\n",
       " 0.004961808731211811,\n",
       " 0.004855064268479689,\n",
       " 0.004750687618388413,\n",
       " 0.004648624477172,\n",
       " 0.004548821829552116,\n",
       " 0.004451227917157713,\n",
       " 0.004355792207741335,\n",
       " 0.004262465365172469,\n",
       " 0.0041711992201853565,\n",
       " 0.00408194674186535,\n",
       " 0.003994662009851713,\n",
       " 0.003909300187239562,\n",
       " 0.0038258174941632472,\n",
       " 0.003744171182043149,\n",
       " 0.0036643195084786334,\n",
       " 0.00358622171277094,\n",
       " 0.003509837992059825,\n",
       " 0.003435129478057755,\n",
       " 0.0033620582143661926,\n",
       " 0.0032905871343602983,\n",
       " 0.0032206800396255425,\n",
       " 0.0031523015789334252,\n",
       " 0.0030854172277421515,\n",
       " 0.003019993268208662,\n",
       " 0.0029559967696989042,\n",
       " 0.002893395569783755,\n",
       " 0.0028321582557081365,\n",
       " 0.0027722541463210265,\n",
       " 0.002713653274454886,\n",
       " 0.002656326369742959,\n",
       " 0.0026002448418627616,\n",
       " 0.0025453807641958176,\n",
       " 0.0024917068578921703,\n",
       " 0.002439196476329869,\n",
       " 0.0023878235899593425,\n",
       " 0.0023375627715221114,\n",
       " 0.0022883891816359025,\n",
       " 0.0022402785547348814,\n",
       " 0.0021932071853579538,\n",
       " 0.0021471519147742563,\n",
       " 0.0021020901179394647,\n",
       " 0.0020579996907726106,\n",
       " 0.002014859037746425,\n",
       " 0.0019726470597829088,\n",
       " 0.0019313431424461958,\n",
       " 0.001890927144426055,\n",
       " 0.0018513793863032674,\n",
       " 0.001812680639591016,\n",
       " 0.0017748121160450214,\n",
       " 0.0017377554572345355,\n",
       " 0.0017014927243700767,\n",
       " 0.0016660063883781884,\n",
       " 0.0016312793202202663,\n",
       " 0.0015972947814467398,\n",
       " 0.0015640364149821367,\n",
       " 0.0015314882361347204,\n",
       " 0.0014996346238252828,\n",
       " 0.0014684603120292683,\n",
       " 0.0014379503814268974,\n",
       " 0.0014080902512571408,\n",
       " 0.0013788656713683012,\n",
       " 0.0013502627144623691,\n",
       " 0.00132226776852723,\n",
       " 0.0012948675294523065,\n",
       " 0.0012680489938228096,\n",
       " 0.001241799451888445,\n",
       " 0.0012161064807024724,\n",
       " 0.001190957937425824,\n",
       " 0.001166341952793415,\n",
       " 0.0011422469247378692,\n",
       " 0.0011186615121667187,\n",
       " 0.001095574628889956,\n",
       " 0.001072975437693205,\n",
       " 0.0010508533445537233,\n",
       " 0.0010291979929952788,\n",
       " 0.0010079992585785985]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
