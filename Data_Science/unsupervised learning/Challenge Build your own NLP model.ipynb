{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import product_reviews_2, stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying products using text reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attemt to use bag of words and TF-IDF to create features for prduct reviews that were downloaded from the NLTK database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Canon_PowerShot_SD500.txt',\n",
       " 'Canon_S100.txt',\n",
       " 'Diaper_Champ.txt',\n",
       " 'Hitachi_router.txt',\n",
       " 'Linksys_Router.txt',\n",
       " 'MicroMP3.txt',\n",
       " 'Nokia_6600.txt',\n",
       " 'README.txt',\n",
       " 'ipod.txt',\n",
       " 'norton.txt']"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_reviews_2.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews\n",
    "Canon_PowerShot_SD500 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Canon_S100=  product_reviews_2.raw( 'Canon_S100.txt')\n",
    "Diaper_Champ = product_reviews_2.raw('Diaper_Champ.txt')\n",
    "Hitachi_route =  product_reviews_2.raw('Hitachi_router.txt')\n",
    "Linksys_Router = product_reviews_2.raw('Linksys_Router.txt')\n",
    "MicroMP3 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Nokia_6600 = product_reviews_2.raw('Nokia_6600.txt')\n",
    "ipod = product_reviews_2.raw('ipod.txt')\n",
    "norton = product_reviews_2.raw('norton.txt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[t]\\nSD500[+2]##We really enjoyed shooting with the Canon PowerShot SD500. \\ndesign[+2]##It has an exterior design that combines form and function more elegantly than any point-and-shoot we\\'ve ever tested. \\nimage-processing system[+1]##A Digic II-powered image-processing system enables the SD500 to snap a limitless stream of 7-megapixel photos at a respectable clip, its start-up time is tops in its class, and it delivers decent photos when compared to its competition. \\nimage[+2]##The SD500 rivals the Canon G6 in image quality. \\n##I\\'ve had it for about a month and it is simply the best point-and-shoot your money can buy. \\n##I sincerely question the CNET reviewer\\'s ability and/or their bias. \\n##If you want something resembling a REAL and professional review before plunking down your $500.00, I suggest going to dpreview or dcresource and reading the reviews there. \\n##I post under JTL at dcresource...check out my images if you want to see what this camera can do. \\npricey[-1]##I thought given all the \"10\" reviews about this camera that I would buy it in spite of being a bit pricey. \\nLCD[-1]##I did see the review here and other places about a cracked LCD screen. \\n##I figured that the few people who reported that the LCD screen cracked on them was episodic and it would never happen to me. \\n##I received the SD500 on Monday, charged the battery and inserted the SD card that came with it, took a few pictures and waited until Thursday when the 2 GB SD card came that I had ordered. \\n##The camera was sitting on my home desk waiting for the new digital card and the weekend for the first round of serious shooting. \\n##It was NOT dropped or sat on or subjected to any other trauma -- it just sat on my desk in my home office \\\\u00F4 where no one goes but me. \\nLCD[-1]##When I went to put in the new SD Card I looked at the LCD Screen -- CRACKED-- and the fluid gave a Jackson Pollock type appearance to the screen, black and blue and purple blotches and streaks --useless. \\n##I have called Ca'"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Canon_PowerShot_SD500[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little cleaning\n",
    "def clean_txt(text):    \n",
    "    text = text.lower() \n",
    "    text  = re.sub(r'^[^ ]*', '', text) # removes single letters\n",
    "    text = re.sub(\"[^a-zA-Z.' ]+\", \" \", text) #removes anything that isn't a letter\n",
    "    text = re.sub('\\[.*?\\]', \" \", text)# removes text and brackets if there are words or letters in brackets\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)# removes breaks\n",
    "    text = \" \".join(text.split()).strip()\n",
    "\n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning to all documents\n",
    "Canon_PowerShot_SD500 = clean_txt(Canon_PowerShot_SD500[:int(len(Canon_PowerShot_SD500)/10)])\n",
    "Canon_S100 = clean_txt(Canon_S100[:int(len(Canon_S100)/10)])\n",
    "Diaper_Champ = clean_txt(Diaper_Champ[:int(len(Diaper_Champ)/5)])\n",
    "Hitachi_route = clean_txt(Hitachi_route[:int(len(Hitachi_route)/10)])\n",
    "Linksys_Router = clean_txt(Linksys_Router[:int(len(Linksys_Router)/10)])\n",
    "MicroMP3 = clean_txt(MicroMP3[:int(len(MicroMP3)/10)])\n",
    "Nokia_6600 = clean_txt(Nokia_6600[:int(len(Nokia_6600)/10)])\n",
    "ipod = clean_txt(ipod[:int(len(ipod)/10)])\n",
    "norton = clean_txt(norton[:int(len(norton)/5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse with SpaCy\n",
    "nlp = spacy.load('en')\n",
    "Canon_PowerShot_SD500_doc = nlp(Canon_PowerShot_SD500)\n",
    "Canon_S100_doc = nlp(Canon_S100) \n",
    "Diaper_Champ_doc = nlp(Diaper_Champ) \n",
    "Hitachi_route_doc = nlp(Hitachi_route) \n",
    "Linksys_Router_doc = nlp(Linksys_Router)\n",
    "MicroMP3_doc = nlp(MicroMP3) \n",
    "Nokia_6600_doc = nlp(Nokia_6600) \n",
    "ipod_doc = nlp(ipod) \n",
    "norton_doc = nlp(norton) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "really enjoyed shooting with the canon powershot sd . design it has an exterior design that combines form and function more elegantly than any point and shoot we've ever tested. image processing system a digic ii powered image processing system enables the sd to snap a limitless stream of megapixel photos at a respectable clip its start up time is tops in its class and it delivers decent photos when compared to its competition. image the sd rivals the canon g in image quality. i've had it for about a month and it is simply the best point and shoot your money can buy. i sincerely question the cnet reviewer's ability and or their bias. if you want something resembling a real and professional review before plunking down your . i suggest going to dpreview or dcresource and reading the reviews there. i post under jtl at dcresource...check out my images if you want to see what this camera can do. pricey i thought given all the reviews about this camera that i would buy it in spite of being a bit pricey. lcd i did see the review here and other places about a cracked lcd screen. i figured that the few people who reported that the lcd screen cracked on them was episodic and it would never happen to me. i received the sd on monday charged the battery and inserted the sd card that came with it took a few pictures and waited until thursday when the gb sd card came that i had ordered. the camera was sitting on my home desk waiting for the new digital card and the weekend for the first round of serious shooting. it was not dropped or sat on or subjected to any other trauma it just sat on my desk in my home office u f where no one goes but me. lcd when i went to put in the new sd card i looked at the lcd screen cracked and the fluid gave a jackson pollock type appearance to the screen black and blue and purple blotches and streaks useless. i have called cannon they are sending me an rfa i will let you know if they treat me well or otherwise. camera this camera is truely superb. pictures the startup time quality of pictures and videos is outstanding. manual control some more manual control would have been nice. and shooters in low light should either select the iso manually and or make sure that they don't shake much as the camera's chip goes for detail over smoothness. also ensure tha"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Canon_PowerShot_SD500_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into sentences\n",
    "Canon_PowerShot_SD500_sents =[[sent, \"camera\"] for sent in Canon_PowerShot_SD500_doc.sents]\n",
    "Canon_S100_sents = [[sent, \"camera\"] for sent in Canon_S100_doc.sents]\n",
    "Diaper_Champ_sents = [[sent, \"diaper_machine\"] for sent in Diaper_Champ_doc.sents]\n",
    "Hitachi_route_sents = [[sent, \"router\"] for sent in Hitachi_route_doc.sents]\n",
    "Linksys_Router_sents = [[sent, \"router\"] for sent in Linksys_Router_doc.sents]\n",
    "MicroMP3_sents = [[sent, \"mp3_player\"] for sent in MicroMP3_doc.sents]\n",
    "aNokia_6600_sents = [[sent, \"phone\"] for sent in Nokia_6600_doc.sents]\n",
    "ipod_sents = [[sent, \"mp3_player\"] for sent in ipod_doc.sents]\n",
    "norton_sents = [[sent, \"AV_software\"] for sent in norton_doc.sents]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(really, enjoyed, shooting, with, the, canon, ...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(design, it, has, an, exterior, design, that, ...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(image, processing, system, a, digic, ii, powe...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(image, the, sd, rivals, the, canon, g, in, im...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(i, 've, had, it, for, about, a, month, and, i...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (really, enjoyed, shooting, with, the, canon, ...  camera\n",
       "1  (design, it, has, an, exterior, design, that, ...  camera\n",
       "2  (image, processing, system, a, digic, ii, powe...  camera\n",
       "3  (image, the, sd, rivals, the, canon, g, in, im...  camera\n",
       "4  (i, 've, had, it, for, about, a, month, and, i...  camera"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frame for sentences and target\n",
    "sentences = pd.DataFrame(Canon_PowerShot_SD500_sents+Canon_S100_sents+Diaper_Champ_sents+\n",
    "                         Hitachi_route_sents+Linksys_Router_sents+MicroMP3_sents+aNokia_6600_sents+ipod_sents+norton_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mp3_player        119\n",
       "router            114\n",
       "diaper_machine    102\n",
       "AV_software        94\n",
       "phone              88\n",
       "camera             63\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter and return words\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(200)];\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and data frame\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "       \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appyly functions\n",
    "\n",
    "Canon_PowerShot_SD500_words = bag_of_words(Canon_PowerShot_SD500_doc)\n",
    "Canon_S100_words = bag_of_words(Canon_S100_doc)\n",
    "Diaper_Champ_words = bag_of_words(Diaper_Champ_doc)\n",
    "Hitachi_router_words = bag_of_words(Hitachi_route_doc)\n",
    "Linksys_Router_words = bag_of_words(Linksys_Router_doc)\n",
    "MicroMP3_words = bag_of_words(MicroMP3_doc)\n",
    "Nokia_6600_words = bag_of_words(Nokia_6600_doc)\n",
    "ipod_words  = bag_of_words(ipod_doc)\n",
    "norton_words = bag_of_words(norton_doc)\n",
    "\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(Canon_PowerShot_SD500_words+\n",
    "Canon_S100_words+\n",
    "Diaper_Champ_words+\n",
    "Hitachi_router_words+\n",
    "Linksys_Router_words+\n",
    "MicroMP3_words+\n",
    "Nokia_6600_words+\n",
    "ipod_words+\n",
    "norton_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>toddlerhood</th>\n",
       "      <th>post</th>\n",
       "      <th>idea</th>\n",
       "      <th>unable</th>\n",
       "      <th>literally</th>\n",
       "      <th>hate</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>formula</th>\n",
       "      <th>god</th>\n",
       "      <th>...</th>\n",
       "      <th>proudly</th>\n",
       "      <th>set</th>\n",
       "      <th>x</th>\n",
       "      <th>know</th>\n",
       "      <th>static</th>\n",
       "      <th>gas</th>\n",
       "      <th>log</th>\n",
       "      <th>class</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(really, enjoyed, shooting, with, the, canon, ...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(design, it, has, an, exterior, design, that, ...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(image, processing, system, a, digic, ii, powe...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(image, the, sd, rivals, the, canon, g, in, im...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, 've, had, it, for, about, a, month, and, i...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 941 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  kind toddlerhood post idea unable literally hate smoothness formula god  \\\n",
       "0    0           0    0    0      0         0    0          0       0   0   \n",
       "1    0           0    0    0      0         0    0          0       0   0   \n",
       "2    0           0    0    0      0         0    0          0       0   0   \n",
       "3    0           0    0    0      0         0    0          0       0   0   \n",
       "4    0           0    0    0      0         0    0          0       0   0   \n",
       "\n",
       "      ...     proudly set  x know static gas log class  \\\n",
       "0     ...           0   0  0    0      0   0   0     0   \n",
       "1     ...           0   0  0    0      0   0   0     0   \n",
       "2     ...           0   0  0    0      0   0   0     1   \n",
       "3     ...           0   0  0    0      0   0   0     0   \n",
       "4     ...           0   0  0    0      0   0   0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (really, enjoyed, shooting, with, the, canon, ...      camera  \n",
       "1  (design, it, has, an, exterior, design, that, ...      camera  \n",
       "2  (image, processing, system, a, digic, ii, powe...      camera  \n",
       "3  (image, the, sd, rivals, the, canon, g, in, im...      camera  \n",
       "4  (i, 've, had, it, for, about, a, month, and, i...      camera  \n",
       "\n",
       "[5 rows x 941 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features.  \n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Random forest and linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7333333333333333\n",
      "\n",
      "Test set score: 0.5586206896551724\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=200,max_depth=10,)\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is some overfitting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keith\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435, 939) (435,)\n",
      "Training set score: 0.9379310344827586\n",
      "\n",
      "Test set score: 0.6137931034482759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class='auto') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way more overfitting with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "Canon_PowerShot_SD500 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Canon_S100=  product_reviews_2.raw( 'Canon_S100.txt')\n",
    "Diaper_Champ = product_reviews_2.raw('Diaper_Champ.txt')\n",
    "Hitachi_route =  product_reviews_2.raw('Hitachi_router.txt')\n",
    "Linksys_Router = product_reviews_2.raw('Linksys_Router.txt')\n",
    "MicroMP3 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Nokia_6600 = product_reviews_2.raw('Nokia_6600.txt')\n",
    "ipod = product_reviews_2.raw('ipod.txt')\n",
    "norton = product_reviews_2.raw('norton.txt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into sentences\n",
    "Canon_PowerShot_SD500_sents =[[sent, \"camera\"] for sent in Canon_PowerShot_SD500.split(\".\")]\n",
    "Canon_S100_sents = [[sent, \"camera\"] for sent in Canon_S100.split(\".\")]\n",
    "Diaper_Champ_sents = [[sent, \"diaper_machine\"] for sent in Diaper_Champ.split(\".\")]\n",
    "Hitachi_route_sents = [[sent, \"router\"] for sent in Hitachi_route.split(\".\")]\n",
    "Linksys_Router_sents = [[sent, \"router\"] for sent in Linksys_Router.split(\".\")]\n",
    "MicroMP3_sents = [[sent, \"mp3_player\"] for sent in MicroMP3.split(\".\")]\n",
    "aNokia_6600_sents = [[sent, \"phone\"] for sent in Nokia_6600.split(\".\")]\n",
    "ipod_sents = [[sent, \"mp3_player\"] for sent in ipod.split(\".\")]\n",
    "norton_sents = [[sent, \"AV_software\"] for sent in norton.split(\".\")]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[t]\\nSD500[+2]##We really enjoyed shooting wit...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ndesign[+2]##It has an exterior design that ...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nimage-processing system[+1]##A Digic II-pow...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nimage[+2]##The SD500 rivals the Canon G6 in...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n##I've had it for about a month and it is s...</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  [t]\\nSD500[+2]##We really enjoyed shooting wit...  camera\n",
       "1   \\ndesign[+2]##It has an exterior design that ...  camera\n",
       "2   \\nimage-processing system[+1]##A Digic II-pow...  camera\n",
       "3   \\nimage[+2]##The SD500 rivals the Canon G6 in...  camera\n",
       "4   \\n##I've had it for about a month and it is s...  camera"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine sentences in to data frame\n",
    "tech = pd.DataFrame(Canon_PowerShot_SD500_sents+Canon_S100_sents+Diaper_Champ_sents+\n",
    "                         Hitachi_route_sents+Linksys_Router_sents+MicroMP3_sents+aNokia_6600_sents+ipod_sents+norton_sents)\n",
    "tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1675\n",
      "Original sentence:  \n",
      "##I sincerely question the CNET reviewer's ability and/or their bias\n",
      "Tf_idf vector: {'router': 0.21363780075707028, 'hard': 0.30772385619621256, 'good': 0.21535669144366698, 'chuck': 0.41087857655521154, 'lock': 0.35930121637571205, 'wrench': 0.4270719978699523, 'collet': 0.3880552310889314, 'loosen': 0.4270719978699523}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(tech[0],tech[1], test_size=0.4, random_state=0)\n",
    "\n",
    "# function to convert to lemmas\n",
    "def lemmas(text):\n",
    "    text = nlp(text)\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    return allwords;\n",
    "    \n",
    "\n",
    "# create TFIDF object\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmas, # get lemmas from SpaCy\n",
    "                             max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "print(\"Number of features: %d\" % X_train_tfidf.get_shape()[1])\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "  \n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 42.10922116331199\n",
      "Component 0:\n",
      "0\n",
      " \\n## I disagree                                              0.560348\n",
      " \\nVersatile[+2]##Versatile and stylish                       0.560348\n",
      " \\nSpeed[+2][u],signal[+2],Configuration Options[+2]##Pros    0.560348\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      " \\nVersatile[+2]##Versatile and stylish    0.637786\n",
      " \\n##More detail                           0.637786\n",
      " \\n## I disagree                           0.637786\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      " another                       0.659732\n",
      " does more than most           0.659732\n",
      "\\n##  Again, another hassle    0.653401\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "\\n[t]\\nphone[+3]##This phone is one of the milestones in the history of the cellular phones                   0.560528\n",
      "\\n[t]\\nphone[+3]##this is a great phone i have this phone since Feb 04 and i never have any problem at all    0.523857\n",
      "\\n[t]\\n##I bought this phone after having other phones that had less-than-consistant reception                0.519828\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "  \\n## I'm not one of them                              0.591008\n",
      "  \\nConfiguration[-2],Software[-2],UPnP[-2]## ##Cons    0.591008\n",
      "  \\n##I'm a male and that's what we do (or not do)      0.591008\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0  \n",
      "##If you have users that use P2P, you will need to tweak this and all wireless routers\n",
      "1   \n",
      "Wizard[-2][u] ##The Wizard is particularly sinister\n",
      "2  \n",
      "## Apple is already being outpaced by Creative Labs and Sony on the battery front, and just about any non-Apple player allows you more compatibility with download services than Apple's iTunes-only program\n",
      "3 \n",
      "heavy[-1][u]##Only problem is that is a bit heavy\n",
      "4 \n",
      "##I am SO thankful I got that advice when I did because I had the Diaper Genie on my registry aforementioned to her warning\n",
      "5 \n",
      "grip[-1],chuck[-1],collet[-1]##It's oftentimes hard to get a good grip on the router, activate the chuck lock, and get enough muscle behind the wrench to get the collet loosened up\n",
      "6 \n",
      "[t]\n",
      "S100[+3][cc]##This is my second digital camera, but I'm much more impressed with the Canon S100 Digital Elph than I EVER was with my Kodak DC240\n",
      "7 \n",
      "size[+1]##look at the size of it!\n",
      "##Any diaper pail, if you leave it for long enough, is going to smell\n",
      "8   \n",
      "##It may take several attempts, and reading a lot of FAQs or network help sites to fix some of these issues\n",
      "9  \n",
      "image[+2]##Good image quality\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest and logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8832720588235294\n",
      "\n",
      "Test set score: 0.6336088154269972\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=200,max_depth=10,) \n",
    "train = rfc.fit(X_train_lsa, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test_lsa, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as much overfitting this time. let't look at linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176,) (2176,)\n",
      "Training set score: 0.7325367647058824\n",
      "\n",
      "Test set score: 0.6721763085399449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear',multi_class='auto',) # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_lsa, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742336572833676"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "pred = lr.predict(X_test_lsa)\n",
    "cross_val_score(lr,X_train_lsa,y_train,cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like linear regression with TF-IDF is the best model. Next will be and attemt to increase the score by 5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to increase score by 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll attemt to increase the score by adding some more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canon_PowerShot_SD500 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Canon_S100=  product_reviews_2.raw( 'Canon_S100.txt')\n",
    "Diaper_Champ = product_reviews_2.raw('Diaper_Champ.txt')\n",
    "Hitachi_route =  product_reviews_2.raw('Hitachi_router.txt')\n",
    "Linksys_Router = product_reviews_2.raw('Linksys_Router.txt')\n",
    "MicroMP3 = product_reviews_2.raw('Canon_PowerShot_SD500.txt')\n",
    "Nokia_6600 = product_reviews_2.raw('Nokia_6600.txt')\n",
    "ipod = product_reviews_2.raw('ipod.txt')\n",
    "norton = product_reviews_2.raw('norton.txt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canon_PowerShot_SD500_sents =[[sent, \"camera\"] for sent in Canon_PowerShot_SD500.split(\".\")]\n",
    "Canon_S100_sents = [[sent, \"camera\"] for sent in Canon_S100.split(\".\")]\n",
    "Diaper_Champ_sents = [[sent, \"diaper_machine\"] for sent in Diaper_Champ.split(\".\")]\n",
    "Hitachi_route_sents = [[sent, \"router\"] for sent in Hitachi_route.split(\".\")]\n",
    "Linksys_Router_sents = [[sent, \"router\"] for sent in Linksys_Router.split(\".\")]\n",
    "MicroMP3_sents = [[sent, \"mp3_player\"] for sent in MicroMP3.split(\".\")]\n",
    "aNokia_6600_sents = [[sent, \"phone\"] for sent in Nokia_6600.split(\".\")]\n",
    "ipod_sents = [[sent, \"mp3_player\"] for sent in ipod.split(\".\")]\n",
    "norton_sents = [[sent, \"AV_software\"] for sent in norton.split(\".\")]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(text):    \n",
    "    #text = text.lower() \n",
    "    text = re.sub(r'^[^ ]*', '', text)\n",
    "    text = re.sub(\"[^a-zA-Z.' ]+\", \" \", text)\n",
    "    text = re.sub('\\[.*?\\]', \" \", text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #text = \" \".join(text.split()).strip()\n",
    "\n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0] = sentences[0].apply(lambda x: clean_txt(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = pd.DataFrame(Canon_PowerShot_SD500_sents+Canon_S100_sents+Diaper_Champ_sents+\n",
    "                         Hitachi_route_sents+Linksys_Router_sents+MicroMP3_sents+aNokia_6600_sents+ipod_sents+norton_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1675\n",
      "Original sentence:  \n",
      "##I sincerely question the CNET reviewer's ability and/or their bias\n",
      "Tf_idf vector: {'router': 0.21363780075707028, 'hard': 0.30772385619621256, 'good': 0.21535669144366698, 'chuck': 0.41087857655521154, 'lock': 0.35930121637571205, 'wrench': 0.4270719978699523, 'collet': 0.3880552310889314, 'loosen': 0.4270719978699523}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(tech[0],tech[1], test_size=0.4, random_state=0)\n",
    "\n",
    "# function to convert to lemmas\n",
    "def lemmas(text):\n",
    "    text = nlp(text)\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    return allwords;\n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmas, # get lemmas from SpaCy\n",
    "                             max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "#Applying the vectorizer\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "print(\"Number of features: %d\" % X_train_tfidf.get_shape()[1])\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "  \n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 42.0743067641299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176,) (2176,)\n",
      "Training set score: 0.7734375\n",
      "\n",
      "Test set score: 0.6728650137741047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear',multi_class='auto',C=100000) # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_lsa, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6614373033756467"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "pred = lr.predict(X_test_lsa)\n",
    "cross_val_score(lr,X_train_lsa,y_train,cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
